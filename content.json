{"pages":[{"title":"about","text":"","link":"/about/index.html"},{"title":"archives","text":"","link":"/archives/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"Git基础命令","text":"一 创建版本库123456mkdir learngit (创建文件夹)cd learngitgit init (初始化，创建版本库)touch &lt;file&gt; (新建文件)git add &lt;file&gt; (添加文件)git commit -m &lt;message&gt; (提交文件，输入提交信息) 二 时光机穿梭1. 版本回退12345678git status (查看文件状态)git diff HEAD &lt;file&gt; (查看当前版本和版本库中的差异)git log (查看提交日志)git log --pretty=online (查看日志简洁信息)git reset --hard HEAD (HEAD表示当前版本，HEAD^表示上一个版本，HEAD~100表示第前100个版本)git reset --hard &lt;commit_id&gt; (回退到指定版本号的版本，版本号可以不用写全)git reset HEAD &lt;file&gt; (把暂存区的修改撤销掉)git reflog (查看命令历史) 2. 撤销修改123456git checkout -- &lt;file&gt; (把文件在工作区的修改全部撤销) (1) 修改后还没有放到暂存区，现在撤销修改就回退到和版本库一模一样的状态； (2) 修改后已经放到暂存区，现在撤销修改就回退到添加到暂存区的状态； (3) 针对第二种情况，如果要想回退到和版本库一模一样的状态，执行 git reset --hard HEADgit rm --cached &lt;file&gt; (删除提交到缓存区的文件，回退到没有提交的状态)git reset HEAD &lt;file&gt; (把暂存区的修改撤销掉) 3. 删除文件1234rm &lt;file&gt; (删除工作区的文件,如果想要删除版本库中的文件，就执行2,3语句) (1) git rm &lt;file&gt; (2) git commit -m &lt;message&gt; (3) git checkout -- &lt;file&gt; (如果误删除了，可以使用本条语句恢复到版本库中的状态，但是不能保留已经修改的但未添加到暂存区的部分，其实就是和撤销删除中的第二条一样) 三 远程仓库1. 添加远程仓库12345678ssh-keygen -t rsa -C &quot;youremail@example.com&quot; (创建SSH KEY)执行完上述命令，用户目录会产生一个.ssh目录，里面会有id_rsa和id_rsa.pub两个文件id_rsa是私钥，不能泄露，id_rsa.pub是公钥，可以告诉他人登陆GitHub，打开“Account settings”，“SSH Keys”页面：然后，点“Add SSH Key”，填上任意Title，在Key文本框里粘贴id_rsa.pub文件的内容：点“Add Key”，你就应该看到已经添加的Key：在github上创建一个仓库git remote add origin git@github.com:motifsing/learngit.git (origin是远程仓库的名称，motifsing是github的账户名，learngit是github的仓库名称)git remote rm origin (删除远程仓库)git push -u origin master (首次推送)git push origin master (后续提交) 2. 克隆远程仓库1git clone git@github.com:motifsing/learngit.git 四 分支管理1. 创建与合并分支123456git branch (查看分支)git branch &lt;name&gt; (创建分支)git checkout &lt;name&gt; (切换分支)git checkout -b &lt;name&gt; (创建+切换分支)git merge &lt;name&gt; (合并name分支到当前分支，fast award module)git branch -d &lt;name&gt; (删除分支) 2.解决冲突123git merge 不能快速合并产生冲突的时候，需要在合并状态下，手动修改文件，然后提交git merge --abort (退出合并)git log --graph (查看分支合并图) 3.分支管理策略12git merge --no-ff -m &lt;merge message&gt; &lt;dev name&gt; (普通模式合并，保留合并信息)git log --graph --pretty=oneline --abbrev-commit (查看分支历史) 4.Bug分支123456git stash (储存当前工作去的文件信息，可以多次储存)git stash list (查看当前储存的信息)git stash apply (恢复最后一次储存的信息，但是不删除储存的信息)git stash drop (删除储存的信息)git stash pop (恢复信息，并且删除存储的信息)git stash apply stash{&lt;number&gt;} 恢复指定的信息 5.Feature分支12开发一个新feature，最好新建一个分支git branch -D &lt;name&gt; (强制删除一个未被合并过的分区) 6.多人协作123456git remote -v (查看远程库信息)git checkout -b branch-name origin/branch-name (在本地创建和远程分支对应的分支)git push origin &lt;branch-name&gt; 推动自己的修改到远程仓库的分支git pull (如果推送失败，需要先git pull，合并下远程仓库的文件，如果产生冲突，先解决冲突，再在本地提交)git branch --set-upstream-to &lt;branch-name&gt; origin/&lt;branch-name&gt; (如果git pull提示no tranking information,说明本质分支与远程分支的链接关系没有创建，创建本地分支和远程分支的关联)git push origin &lt;branch-name&gt; (推送本地修改到远程仓库) 五 标签管理1.创建标签1234567切换到需要打标签的分支上git tag (查看所有标签)git show &lt;tagname&gt;git tag &lt;tagname&gt; (默认是HEAD上)git log --pretty=oneline --abbrev-commit (查看提交历史信息)git tag &lt;tagname&gt; &lt;commit_id&gt; (通过提交版本号打标签)git tag -a &lt;ttagname -m &lt;message&gt; (新建标签并且指定标签信息) 2.管理标签123456git tag -d &lt;tagname&gt; (删除标签)git push origin &lt;tagname&gt; (推送一个本地标签到远程仓库)git push origin --tags (推送所有本地尚未推动到远程的标签到远程)删除推送到远程的标签 (1) git tag -d &lt;tagname&gt; (先删除本地标签) (2) git push origin :refs/tags/&lt;tagname&gt; (再从远程删除) 六 自定义Git1.忽略特殊文件1234在git工作区的根目录下新建一个.gitignore文件，然后提交了在线配置文件：https://github.com/github/gitignoregit add -f &lt;file&gt; (强制添加被忽略的文件)git checkout-ignore -v &lt;file&gt; (检查忽略规则，是哪条规则忽略了文件) 2.配置别名1git config --global alias.&lt;alias-name&gt; &lt;command-name&gt;","link":"/2019/07/19/git命令/"},{"title":"Linux升级Python","text":"12345rpm -qa | grep javarpm -e --nodeps java-1.7.0-openjdk-headless-1.7.0.91-2.6.2.3.el7.x86_64rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.65-3.b17.el7.x86_64rpm -e --nodeps java-1.8.0-openjdk-1.8.0.65-3.b17.el7.x86_64rpm -e --nodeps java-1.7.0-openjdk-1.7.0.91-2.6.2.3.el7.x86_64 1234567891011121314151617181920212223242526272829303132333435#安装依赖包yum group install &apos;Development Tools&apos;yum install gcc-c++ gcc make cmake zlib-devel bzip2-devel openssl-devel ncurse-devel -y./configure --prefix=/usr/local/python3 --enable-optimizationsmake &amp;&amp; make installmv /usr/bin/python /usr/bin/python.bakln -s /usr/local/python3/bin/python3.7 /usr/bin/pythonln -s /usr/local/python3/bin/pip3 /usr/bin/pipmkdir ~/.pipvim ~/.pip/pip.conf[global] timeout = 6000 index-url = http://pypi.douban.com/simple/ [install] use-mirrors = true mirrors = http://pypi.douban.com/simple/ trusted-host = pypi.douban.com#修改yumvim /usr/bin/yum将/usr/bin/python改为/usr/bin/python2.7vim /usr/bin/yum-config-manager将/usr/bin/python改为/usr/bin/python2.7vim /usr/libexec/urlgrabber-ext-down将/usr/bin/python改为/usr/bin/python2.7pip install --upgrade pip","link":"/2019/07/19/linux升级Python/"},{"title":"线性回归，岭回归和Lasso回归","text":"线性回归线性回归的一般形式$$\\tag{x是向量}假设函数：h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\dots +\\theta_nx_n=\\theta^Tx\\qquad$$ $$损失函数：J(\\theta) = {1 \\over 2m}\\sum_{i=1}^m\\Big({h_\\theta\\big({x^{(i)}}\\big)-y^{(i)}}\\Big)^2$$ $$目标: 求minJ(\\theta_0,\\theta_1,\\dots,\\theta_n)时的\\theta 值$$ 梯度下降法求解$$\\begin{alignedat}{2} \\frac{\\partial}{\\partial\\theta_j}J(\\theta)= \\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2m}\\sum_{i=1}^m\\Big({h_\\theta\\big({x^{(i)}}\\big)-y^{(i)}}\\Big)^2\\quad\\ =\\frac{1}{m}\\sum_{i=1}^m\\Big(h_\\theta\\big(x^{(i)}\\big)-y^{(i)}\\Big) \\frac{\\partial}{\\partial\\theta_j}(\\theta^Tx)\\ =\\frac{1}{m}\\sum_{i=1}^m\\Big(h_\\theta\\big(x^{(i)}\\big)-y^{(i)}\\Big)\\cdot{x_j^{(i)}}\\qquad \\end{alignedat}$$ $$\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta) \\qquad (\\alpha{为学习率})\\qquad\\qquad\\qquad\\=\\theta_j - \\frac{\\alpha}{m}\\sum_{i=1}^m\\Big(h_\\theta\\big(x^{(i)}\\big)-y^{(i)}\\Big)\\cdot{x_j^{(i)}} \\qquad(1)$$ 向量化上述公式$$\\begin{equation}\\begin{split} &amp;X = \\begin{vmatrix}x^{(1)}\\x^{(2)}\\\\dots\\x^{(m)}\\end{vmatrix}=\\begin{vmatrix}x_0^{(1)} &amp; x_1^{(1)} &amp; \\dots &amp; x_n^{(1)} \\x_0^{(2)} &amp; x_1^{(2)} &amp; \\dots &amp; x_n^{(2)} \\\\dots &amp; \\dots &amp; \\dots &amp; \\dots &amp; \\x_0^{(m)} &amp; x_1^{(m)} &amp; \\dots &amp; x_n^{(m)}\\end{vmatrix}\\&amp;\\ &amp;Y=\\begin{vmatrix}y^{(1)}\\y^{(2)}\\\\dots \\y^{(m)}\\end{vmatrix} \\qquad \\theta=\\begin{vmatrix}\\theta^{(0)}\\\\theta^{(1)}\\\\dots \\\\theta^{(n)}\\end{vmatrix}\\&amp;\\ &amp;先求x\\cdot\\theta并记为A:A=X\\cdot\\theta=\\begin{vmatrix}x_0^{(1)} &amp; x_1^{(1)} &amp; \\dots &amp; x_n^{(1)} \\x_0^{(2)} &amp; x_1^{(2)} &amp; \\dots &amp; x_n^{(2)} \\\\dots &amp; \\dots &amp; \\dots &amp; \\dots &amp; \\x_0^{(m)} &amp; x_1^{(m)} &amp; \\dots &amp; x_n^{(m)}\\end{vmatrix} \\cdot\\begin{vmatrix}\\theta^{(0)}\\\\theta^{(1)}\\\\dots \\\\theta^{(n)}\\end{vmatrix}=\\begin{vmatrix}\\theta_0x_0^{(1)}+\\theta_1x_1^{(1)}+\\dots+\\theta_nx_n^{(1)} \\\\theta_0x_0^{(2)}+\\theta_1x_1^{(2)}+\\dots+\\theta_nx_n^{(2)} \\\\dots \\\\theta_0x_0^{(m)}+\\theta_1x_1^{(m)}+\\dots+\\theta_nx_n^{(m)} \\\\end{vmatrix}\\&amp;\\ &amp;求h\\theta(x)-y并记为E:\\&amp;E = h_\\theta(x)-Y=\\begin{vmatrix}g(A^{(1)}) - y^{(1)} \\g(A^{(2)}) - y^{(2)} \\\\dots \\g(A^{(m)}) - y^{(m)} \\\\end{vmatrix}=\\begin{vmatrix}e^{(1)} \\e^{(2)} \\\\dots \\e^{(m)}\\end{vmatrix}=g(A) - Y \\&amp;\\ \\end{split}\\end{equation}$$ ​$$\\begin{equation}\\begin{split} &amp;\\color{red}把上述公式代入公式(1)中 \\&amp;\\&amp;\\theta_0 := \\theta_0 - \\frac{\\alpha}{m}\\sum_{i=1}^m\\Big(h_\\theta\\big(x^{(i)}\\big)-y^{(i)}\\Big)\\cdot{x_0^{(i)}} \\&amp;=\\theta_0 - \\frac{\\alpha}{m}\\sum_{i=1}^me^{(i)}x_0^{(i)} \\&amp;=\\theta_0 - \\frac{\\alpha}{m}\\cdot(x_0^{(1)},x_0^{(2)},\\dots,x_0^{(m)})\\cdot{E} \\&amp;\\ &amp;同样的可以写出\\theta_j, \\&amp;\\theta_j :=\\theta_j - \\frac{\\alpha}{m}\\cdot(x_j^{(1)},x_j^{(2)},\\dots,x_j^{(m)})\\cdot{E}\\&amp;\\ &amp;综合起来 \\&amp;\\begin{vmatrix}\\theta^{(0)}\\\\theta^{(1)}\\\\dots \\\\theta^{(n)}\\end{vmatrix}:=\\begin{vmatrix}\\theta^{(0)}\\\\theta^{(1)}\\\\dots \\\\theta^{(n)}\\end{vmatrix}-\\frac{\\alpha}{m}\\cdot\\begin{vmatrix}x_0^{(1)},x_0^{(2)},\\dots,x_0^{(m)} \\x_1^{(1)},x_1^{(2)},\\dots,x_1^{(m)} \\\\dots \\x_n^{(1)},x_n^{(2)},\\dots,x_n^{(m)}\\end{vmatrix}\\cdot{E}=\\theta - \\frac{\\alpha}{m}\\cdot{X^T}\\cdot{E}\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\end{split}\\end{equation}$$ $$\\begin{equation}\\begin{split}&amp;综上所述，vectorization后\\theta更新的步骤 \\&amp;(1) 求A=X\\cdot\\theta \\&amp;(2) 求E=g(A)-Y \\&amp;(3) 求\\theta:=\\theta-\\alpha\\cdot{X}^T\\cdot{E} \\&amp;综合起来 \\&amp;\\theta := \\theta-\\alpha\\cdot(\\frac{1}{m})\\cdot{X^T}\\cdot\\Big(g(X\\cdot\\theta)-Y\\Big)\\&amp;\\quad=\\theta - \\frac{\\alpha}{m}\\cdot{X^T}\\cdot(X\\theta-Y) \\end{split}\\end{equation}$$ 正规方程求解$$\\color{black} \\large 通过求解\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=0的方式来求出使得损失函数最小情况下为0时的\\theta值$$ $$\\begin{equation}\\begin{split} &amp;\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=0 \\&amp;\\&amp;J(\\theta) = \\frac{1}{2m}(X\\cdot\\theta-Y)^T(X\\cdot\\theta-Y) \\&amp;\\qquad=\\frac{1}{2m}(Y^TY-Y^TX\\theta-\\theta^TX^TY+\\theta^Tx^TX\\theta)\\&amp;\\&amp;对J(\\theta)进行求导 \\&amp;\\&amp;\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{1}{2m}(\\frac{\\partial{Y^T}Y}{\\partial\\theta}-\\frac{\\partial{Y^T}X\\theta}{\\partial\\theta}-\\frac{\\partial{\\theta^TX^T}Y}{\\partial\\theta}-\\frac{\\partial\\theta^TX^TX\\theta}{\\partial\\theta})\\&amp;\\&amp;\\large\\color{red}求导公式:\\&amp;\\&amp;\\frac{dAB}{dB} = A^T; \\qquad \\frac{dX^TAX}{dX}=2AX \\&amp;\\&amp;(求导第一项)\\&amp;\\&amp;\\qquad\\frac{\\partial{Y^T}Y}{\\partial\\theta}=0 \\&amp;\\&amp;(求导第二项)\\&amp;\\qquad Y^TX\\theta=\\begin{vmatrix}y^{(1)},y^{(2)},\\dots,y^{(m)}\\end{vmatrix}\\begin{vmatrix}x_0^{(1)} &amp; x_1^{(1)} &amp; \\dots &amp; x_n^{(1)} \\x_0^{(2)} &amp; x_1^{(2)} &amp; \\dots &amp; x_n^{(2)} \\\\dots &amp; \\dots &amp; \\dots &amp; \\dots &amp; \\x_0^{(m)} &amp; x_1^{(m)} &amp; \\dots &amp; x_n^{(m)}\\end{vmatrix}\\begin{vmatrix}\\theta^{(0)},\\theta^{(1)}, \\dots, \\theta^{(n)}\\end{vmatrix}^T\\&amp;\\&amp;\\qquad\\qquad\\quad=(x_0^{(1)}y^{(1)}+\\dots+x_0^{(m)}y^{(m)})\\theta_0 + (x_1{(1)}y^{(1)}+\\dots+x_1{(m)}y^{(m)})\\theta_1 + \\dots + (x_n{(1)}y^{(1)}+\\dots+x_n{(m)}y^{(m)})\\theta_n\\&amp;\\&amp;\\qquad\\frac{\\partial{Y^TX\\theta}}{\\partial\\theta}=\\begin{vmatrix}\\frac{\\partial{Y^TX\\theta}}{\\partial\\theta_0} \\\\dots \\\\frac{\\partial{Y^TX\\theta}}{\\partial\\theta_n}\\end{vmatrix}=\\begin{vmatrix}x_0^{(1)}y^{(1)}+\\dots+x_0^{(m)}y^{(m)} \\\\dots \\x_n^{(1)}y^{(1)}+\\dots+x_n^{(m)}y^{(m)}\\end{vmatrix}=X^TY \\&amp;\\&amp;(求导第三项)\\&amp; \\&amp;\\qquad\\theta^TX^TY=\\begin{vmatrix}\\theta^{(0)}, \\theta^{(1)}, \\dots, \\theta^{(n)}\\end{vmatrix}\\begin{vmatrix}x_0^{(1)} &amp; x_1^{(1)} &amp; \\dots &amp; x_n^{(1)} \\x_0^{(2)} &amp; x_1^{(2)} &amp; \\dots &amp; x_n^{(2)} \\\\dots &amp; \\dots &amp; \\dots &amp; \\dots &amp; \\x_0^{(m)} &amp; x_1^{(m)} &amp; \\dots &amp; x_n^{(m)}\\end{vmatrix}^T\\begin{vmatrix}y^{(1)},y^{(2)},\\dots,y^{(m)}\\end{vmatrix}^T \\&amp; \\&amp;\\qquad\\qquad\\quad=(x_0^{(1)}\\theta_0+\\dots+x_n^{(1)}\\theta_n)y^{(1)} + (x_0^{(2)}\\theta_0+\\dots+x_n^{(2)}\\theta_n)y^{(2)} + \\dots +(x_0^{(m)}\\theta_0+\\dots+x_n^{(m)}\\theta_n)y^{(m)} \\&amp;\\&amp;该矩阵求导为分母布局下的标量/向量形式：\\&amp;\\&amp;\\qquad\\frac{\\partial\\theta^TX^TY}{\\partial\\theta}=\\begin{vmatrix}\\frac{\\partial\\theta^TX^TY}{\\partial\\theta_0} \\\\dots \\\\frac{\\partial\\theta^TX^TY}{\\partial\\theta_n}\\end{vmatrix}=\\begin{vmatrix}x_0^{(1)}y^{(1)}+\\dots+x_0^{(m)}y^{(m)} \\\\dots \\x_n^{(1)}y^{(1)}+\\dots+x_n^{(m)}y^{(m)}\\end{vmatrix}=X^TY \\&amp;\\&amp;(求导第四项)\\&amp; \\&amp;\\qquad \\theta^TX^TX\\theta = (X^TX)(\\theta_0^2 + \\theta_1^2 + \\dots + \\theta_n^2) \\&amp; \\&amp; 其中\\theta^TX^TX\\theta为标量，可看成一个常数，矩阵求导为分母布局下的标量/向量形式：\\&amp; \\&amp;\\qquad \\frac{\\partial\\theta^TX^TX\\theta}{\\partial\\theta}= \\begin{vmatrix}\\frac{\\partial\\theta^TX^TX\\theta}{\\partial\\theta_0} \\\\dots \\\\frac{\\partial\\theta^TX^TX\\theta}{\\partial\\theta_n}\\end{vmatrix}= 2(X^TX)\\begin{vmatrix}\\theta^{(0)}\\\\theta^{(1)}\\\\dots \\\\theta^{(n)}\\end{vmatrix}=2X^TX\\theta \\&amp;\\&amp;综上，正规方程为：\\&amp; \\&amp;\\qquad \\frac{1}{2m}(-2X^TY + 2X^TX\\theta) = 0&amp;\\&amp;特征参数表示如下：\\&amp;\\&amp;\\qquad \\theta=(X^TX)^{-1}X^Ty=0 \\qquad(X为m*(n+1)矩阵) \\ \\end{split}\\end{equation}$$ 岭回归与Lasso回归岭回归的一般形式$$\\tag{x是向量}假设函数：h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\dots +\\theta_nx_n=\\theta^Tx$$ $$岭回归的损失函数：J(\\theta)=\\frac{1}{2m}\\sum_{i=1}^m\\Big(h_\\theta\\big(x^{(i)}\\big)-y^{(i)}\\Big)^2 + \\lambda \\sum_{j=1}^n\\theta_j^2 \\quad$$ $$Lasso回归的损失函数：J(\\theta)={1 \\over 2m}\\sum_{i=1}^m\\Big(h_\\theta\\big(x^{(i)}\\big)-y^{(i)}\\Big)^2 + \\lambda\\sum_{j=1}^n|\\theta_j|$$ $$目标: 求minJ(\\theta_0,\\theta_1,\\dots,\\theta_n)时的\\theta 值$$ $$\\color{red} \\Big (其中\\lambda为正则化参数，因为不对\\theta_0进行惩罚，所以J的值是从1开始)$$ 1岭回归与Lasso回归最大的区别在于岭回归引入的是L2范数惩罚项，Lasso回归引入的是L1范数惩罚项，Lasso回归能够使得损失函数中的许多θ均变成0，这点要优于岭回归，因为岭回归是要所有的θ均存在的，这样计算量Lasso回归将远远小于岭回归。 岭回归梯度下降法$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta) = \\frac{\\partial}{\\partial\\theta_j}\\cdot\\frac{1}{2m}\\Bigg[\\sum_{i=1}^m\\Big(h_\\theta\\big(x^{(i)}\\big)-y^{(i)}\\Big)^2 + \\lambda \\sum_{j=1}^n\\theta_j^2 \\Bigg]\\=\\frac{1}{m}\\sum_{i=1}^m\\Big(h_\\theta\\big(x^{(i)}\\big)-y^{(i)}\\Big)\\cdot{X_j^{(i)}} + \\frac{\\lambda}{m}\\theta_j\\tag{1}$$ $$\\color{red} \\Large \\bf {因为\\theta_0不参与惩罚，因而分为两部分\\qquad\\quad}$$ $$\\theta_0 := \\theta_0 - \\alpha\\sum_{i=1}^m\\Big(h_\\theta\\big(x^{(i)}\\big)-y^{(i)}\\Big)X_0^{(i)}\\tag{2}$$ $$\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)\\tag{3}$$ $${把(1)带入(3)中}:\\\\theta_j := \\theta_j - \\frac{\\alpha}{m}\\sum_{i=1}^m\\Big(h_\\theta\\big(x^{(i)}\\big)-y^{(i)}\\Big)\\cdot{X_j}^{(i)} - \\frac{\\alpha\\lambda}{m}\\theta_j\\:= (1 - \\frac{\\alpha\\lambda}{m})\\theta_j - \\frac{\\alpha}{m}\\sum_{i=1}^m\\Big(h_\\theta\\big(x^{(i)}\\big)-y^{(i)}\\Big)\\cdot{X_j^{(i)}}\\若m训练样本较大时，\\frac{\\lambda}{m}趋向于0，则每次均会进行微调$$ 正规方程求解法$$\\theta = \\Biggr(X^TX + \\lambda\\begin{vmatrix} 0 &amp; &amp; &amp; &amp; \\ &amp; 1 &amp; &amp; &amp; \\ &amp; &amp; . &amp; &amp; \\ &amp; &amp; &amp; . &amp; \\ &amp; &amp; &amp; &amp; 1 \\\\end{vmatrix}\\Biggr)^{-1}\\cdot{X^T}y$$ 可能存在的问题12345678910111：过拟合问题 a.通过正则化（特征缩放）方式来避免 1.提升模型的收敛速度 2.提升模型的精度 b.丢弃一些对我们最终预测结果影响不大的特征，具体哪些特征需要丢弃可以通过PCA算法来实现2：学习率α的选择 a.选取过小，导致迭代次数变多，收敛速度变慢 b.选取过大，有可能跳过最优解，最终导致无法收敛3：X^T*X有可能是奇异矩阵 1.特征量之间存在线性相关性（丢弃一些不必要的特征变量） 2.特征量的个数大于样本个数（增加样本数量，通过正则化数据，加入系数惩罚项） $$X^TX + \\lambda\\begin{vmatrix} 0 &amp; &amp; &amp; &amp; \\ &amp; 1 &amp; &amp; &amp; \\ &amp; &amp; . &amp; &amp; \\ &amp; &amp; &amp; . &amp; \\ &amp; &amp; &amp; &amp; 1 \\\\end{vmatrix}可证明其不可能是奇异矩阵或者退化矩阵，解决一般线性回归正规方程解中出现的奇异矩阵$$ 知识点总结$$\\color{blue}\\begin{equation}\\begin{split}&amp;1. 梯度下降法优化：特征缩放 令x_n = \\frac{x_n-\\mu_n}{s_n} \\qquad （s_n为最大值减去最小值）\\&amp;2. 学习率\\alpha的选择可考虑为：0.01，0.03，0.1，0.3，1.3 \\&amp;3. 绘制损失函数和迭代次数的图像来观测何时收敛 \\&amp;4. 线性回归的衍生版，把多项式转化为线性回归的方式，然后进行特征缩放\\end{split}\\end{equation}$$ python代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import numpy as npclass LinearRegression: def __init__(self): self._theta = None self.coef_ = None self.intercept_ = None # 正规方程解 def normal_equation(self, X_train, y_train): # np.hstack 行合并两个矩阵, np.linalg.inv计算矩阵的逆 X_b = np.hstack((np.ones((len(X_train), 1)), X_train)) self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train) self.intercept_ = self._theta[0] self.coef_ = self._theta[1:] return self # 均值归一化处理数据 def mean_normalization(self, data): \"\"\"均值归一化数据,特征变量的值减去变量的均值然后除以该变量的最大值与最小值的差值\"\"\" return (data - data.mean(axis=0))/(data.max(axis=0) - data.min(axis=0)) def fit_gd(self, X_train, y_train, alpha=0.01, deviation=1e-8, max_item=1e4): assert len(X_train) == len(y_train), \\ \"\"\"The number of samples for X_train should be equal to the number of samples for y_train.\"\"\" # 损失函数 def J(theta, X_b, y): return np.sum((y - X_b.dot(theta)) ** 2) / 2*len(X_b) # 对theta求导 def dJ(theta, X_b, y): return X_b.T.dot((X_b.dot(theta)-y))/len(X_b) def gradient_descent(initial_theta, X_b, y, deviation=1e-8, max_item=1e4): theta = initial_theta item_num = 0 while item_num &lt;= max_item: res = dJ(theta, X_b, y) last_theta = theta theta = theta - alpha*res if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt;= deviation: break item_num += 1 return theta X_b = np.hstack((np.ones((len(X_train), 1)), X_train)) initial_theta = np.zeros(X_b.shape[1]) self._theta = gradient_descent(initial_theta, X_b, y_train, deviation, max_item) self.coef_ = self._theta[1:] self.intercept_ = self._theta[0] return self","link":"/2019/07/19/LinearRegression/"}],"tags":[{"name":"git","slug":"git","link":"/tags/git/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"machine-learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"python, math","slug":"python-math","link":"/tags/python-math/"}],"categories":[{"name":"git","slug":"git","link":"/categories/git/"},{"name":"linux","slug":"linux","link":"/categories/linux/"},{"name":"machine-learning","slug":"machine-learning","link":"/categories/machine-learning/"}]}